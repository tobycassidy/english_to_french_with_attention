{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "congressional-opinion",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import re \n",
    "import os \n",
    "import unicodedata\n",
    "import zipfile\n",
    "import data_load\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cheap-institution",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "alleged-guitar",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning directory data...\n"
     ]
    }
   ],
   "source": [
    "data_load.clean_dir('data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "prerequisite-natural",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_SENT_PAIRS = 30000\n",
    "\n",
    "download_url = \"http://www.manythings.org/anki/fra-eng.zip\"\n",
    "en_sents, fr_sents_in, fr_sents_out = data_load.download_and_read_url(download_url, num_sent_pairs=NUM_SENT_PAIRS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "loose-drove",
   "metadata": {},
   "source": [
    "### Attention layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "amino-reach",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdditiveAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_units):\n",
    "        super(AdditiveAttention, self).__init__()\n",
    "        self.W1 = tf.keras.layers.Dense(num_units)\n",
    "        self.W2 = tf.keras.layers.Dense(num_units)\n",
    "        self.V  = tf.keras.layers.Dense(1)\n",
    "        \n",
    "    def call(self, query, values):\n",
    "        query_with_timesteps = tf.expand_dims(query, axis=1)\n",
    "        score = self.V(\n",
    "            tf.keras.activations.tanh(self.W1(query_with_timesteps) + self.W2(values))\n",
    "        )\n",
    "        alignment = tf.nn.softmax(score, axis=1)\n",
    "        context = tf.reduce_sum(\n",
    "            tf.linalg.matmul(\n",
    "                tf.linalg.matrix_transpose(alignment),\n",
    "                values\n",
    "            ), axis=1\n",
    "        )\n",
    "        context = tf.expand_dims(context, axis=1)\n",
    "        return context, alignment \n",
    "    \n",
    "class Encoder(tf.keras.models.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, num_timesteps, encoder_dim, **kwargs):\n",
    "        super(Encoder, self).__init__(**kwargs)\n",
    "        self.encoder_dim = encoder_dim\n",
    "        self.embedding = tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=num_timesteps)\n",
    "        self.rnn = tf.keras.layers.GRU(encoder_dim, return_sequences=True, return_state=True)\n",
    "        \n",
    "    def call(self, x, state):\n",
    "        x = self.embedding(x)\n",
    "        x, state = self.rnn(x, initial_state=state)\n",
    "        return x, state\n",
    "    \n",
    "    def init_state(self, batch_size):\n",
    "        return tf.zeros((batch_size, self.encoder_dim))\n",
    "    \n",
    "class Decoder(tf.keras.models.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, num_timesteps, decoder_dim, **kwargs):\n",
    "        super(Decoder, self).__init__(**kwargs)\n",
    "        self.decoder_dim = decoder_dim\n",
    "        self.embedding = tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=num_timesteps)\n",
    "        self.attention = AdditiveAttention(embedding_dim)\n",
    "        self.rnn = tf.keras.layers.GRU(decoder_dim, return_sequences=True, return_state=True)\n",
    "        self.Wc = tf.keras.layers.Dense(decoder_dim, activation='tanh')\n",
    "        self.Ws = tf.keras.layers.Dense(vocab_size)\n",
    "        \n",
    "    def call(self, x, state, encoder_out):\n",
    "        x = self.embedding(x)\n",
    "        context, alignment = self.attention(x, encoder_out)\n",
    "        x = tf.expand_dims(\n",
    "            tf.concat([\n",
    "                x, tf.squeeze(context, axis=1)\n",
    "            ], axis=1),\n",
    "        axis=1)\n",
    "        x, state = self.rnn(x, state)\n",
    "        x = self.Wc(x)\n",
    "        x = self.Ws(x)\n",
    "        return x, state, alignment\n",
    "    \n",
    "def loss_fn(ytrue, ypred):\n",
    "    scce = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "    mask = tf.math.logical_not(tf.math.equal(ytrue, 0))\n",
    "    mask = tf.cast(mask, dtype=tf.int64)\n",
    "    loss = scce(ytrue, ypred, sample_weight=mask)\n",
    "    return loss\n",
    "\n",
    "@tf.function\n",
    "def train_step(encoder_in, decoder_in, decoder_out, encoder_state):\n",
    "    with tf.GradientTape() as tape:\n",
    "        encoder_out, encoder_state = encoder(encoder_in, encoder_state)\n",
    "        decoder_state = encoder_state\n",
    "        \n",
    "        loss = 0\n",
    "        for t in range(decoder_out.shape[1]):\n",
    "            decoder_in_t = decoder_in[:, t]\n",
    "            decoder_pred_t, decoder_state, _ = decoder(decoder_in_t, decoder_state, encoder_out)\n",
    "            loss += loss_fn(decoder_out[:, t], decoder_pred_t)\n",
    "    \n",
    "    trainable_variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "    gradients = tape.gradient(loss, trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, trainable_variables))\n",
    "    return loss / decoder_out.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "missing-candy",
   "metadata": {},
   "source": [
    "### Tokenization and data set up "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "manual-projection",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seqlen (en): 8, seqlen (fr): 16\n"
     ]
    }
   ],
   "source": [
    "en_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters=\"\", lower=False)\n",
    "en_tokenizer.fit_on_texts(en_sents)\n",
    "en_data = en_tokenizer.texts_to_sequences(en_sents)\n",
    "en_data = tf.keras.preprocessing.sequence.pad_sequences(en_data, padding='post')\n",
    "\n",
    "fr_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters=\"\", lower=False)\n",
    "fr_tokenizer.fit_on_texts(fr_sents_in)\n",
    "fr_tokenizer.fit_on_texts(fr_sents_out)\n",
    "fr_data_in = fr_tokenizer.texts_to_sequences(fr_sents_in)\n",
    "fr_data_in = tf.keras.preprocessing.sequence.pad_sequences(fr_data_in, padding='post')\n",
    "fr_data_out = fr_tokenizer.texts_to_sequences(fr_sents_out)\n",
    "fr_data_out = tf.keras.preprocessing.sequence.pad_sequences(fr_data_out, padding='post')\n",
    "\n",
    "en_maxlen = en_data.shape[1]\n",
    "fr_maxlen = fr_data_out.shape[1]\n",
    "print(f\"seqlen (en): {en_maxlen}, seqlen (fr): {fr_maxlen}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "biological-checklist",
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 32\n",
    "ENCODER_DIM, DECODER_DIM = 64, 64\n",
    "BATCH_SIZE = 16\n",
    "NUM_EPOCHS = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "passing-professor",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab size (en): 4354, vocab size (fr): 7635\n"
     ]
    }
   ],
   "source": [
    "tf.random.set_seed(123)\n",
    "\n",
    "batch_size = BATCH_SIZE\n",
    "dataset = tf.data.Dataset.from_tensor_slices((en_data, fr_data_in, fr_data_out))\n",
    "dataset = dataset.shuffle(10000)\n",
    "test_size = NUM_SENT_PAIRS // 4\n",
    "test_dataset = dataset.take(test_size).batch(batch_size, drop_remainder=True)\n",
    "train_dataset = dataset.skip(test_size).batch(batch_size, drop_remainder=True)\n",
    "\n",
    "en_vocab_size = len(en_tokenizer.word_index)\n",
    "fr_vocab_size = len(fr_tokenizer.word_index)\n",
    "en_word2idx = en_tokenizer.word_index\n",
    "en_idx2word = {idx:word for word, idx in en_word2idx.items()}\n",
    "fr_word2idx = fr_tokenizer.word_index\n",
    "fr_idx2word = {idx:word for word, idx in fr_word2idx.items()}\n",
    "print(f\"vocab size (en): {en_vocab_size}, vocab size (fr): {fr_vocab_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "circular-order",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder(en_vocab_size+1, embedding_dim=EMBEDDING_DIM, num_timesteps=en_maxlen, encoder_dim=ENCODER_DIM)\n",
    "decoder = Decoder(fr_vocab_size+1, embedding_dim=EMBEDDING_DIM, num_timesteps=fr_maxlen, decoder_dim=DECODER_DIM)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "administrative-pizza",
   "metadata": {},
   "source": [
    "### Test code to examine dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "brazilian-start",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs: (16, 8) (16, 16) (16, 16)\n",
      "encoder input          : (16, 8)\n",
      "encoder output         : (16, 8, 64) state: (16, 64)\n",
      "decoder output (logits): (16, 16, 7636) state: (16, 64)\n",
      "decoder output (labels): (16, 16)\n"
     ]
    }
   ],
   "source": [
    "for encoder_in, decoder_in, decoder_out in train_dataset:\n",
    "    print(\"inputs:\", encoder_in.shape, decoder_in.shape, decoder_out.shape)\n",
    "    encoder_state = encoder.init_state(batch_size)\n",
    "    encoder_out, encoder_state = encoder(encoder_in, encoder_state)\n",
    "    decoder_state = encoder_state\n",
    "    decoder_pred = []\n",
    "    for t in range(decoder_out.shape[1]):\n",
    "        decoder_in_t = decoder_in[:, t]\n",
    "        decoder_pred_t, decoder_state, _ = decoder(decoder_in_t,\n",
    "            decoder_state, encoder_out)\n",
    "        decoder_pred.append(decoder_pred_t.numpy())\n",
    "    decoder_pred = tf.squeeze(np.array(decoder_pred), axis=2)\n",
    "    break\n",
    "print(\"encoder input          :\", encoder_in.shape)\n",
    "print(\"encoder output         :\", encoder_out.shape, \"state:\", encoder_state.shape)\n",
    "print(\"decoder output (logits):\", decoder_pred.shape, \"state:\", decoder_state.shape)\n",
    "print(\"decoder output (labels):\", decoder_out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dimensional-accent",
   "metadata": {},
   "source": [
    "### Training and Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "df5e195b-35dc-4133-97e3-4a3167e06d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(encoder, decoder, batch_size, en_sents, en_data, fr_sents_out, fr_word2idx, fr_idx2word):\n",
    "    random_id = np.random.choice(len(en_sents))\n",
    "    print(\"Input      :    \", \" \".join(en_sents[random_id]))\n",
    "    print(\"Label      :    \", \" \".join(fr_sents_out[random_id]))\n",
    "    \n",
    "    \n",
    "    encoder_in = tf.expand_dims(en_data[random_id], axis=0)\n",
    "    encoder_state = encoder.init_state(1)\n",
    "    encoder_out, encoder_state = encoder(encoder_in, encoder_state)\n",
    "    decoder_state = encoder_state\n",
    "    \n",
    "    fr_pred_sent = []\n",
    "    decoder_in = tf.expand_dims(tf.constant(fr_word2idx[\"BOS\"]), axis=0)\n",
    "    \n",
    "    while True:\n",
    "        decoder_pred, decoder_state, alignment = decoder(decoder_in, decoder_state, encoder_out)\n",
    "        decoder_pred = tf.argmax(decoder_pred, axis=-1)\n",
    "        pred_word = fr_idx2word[decoder_pred.numpy()[0][0]]\n",
    "        fr_pred_sent.append(pred_word)\n",
    "        if pred_word == \"EOS\":\n",
    "            break\n",
    "        decoder_in = tf.squeeze(decoder_pred, axis=1)\n",
    "        \n",
    "    print(\"Predicted  :    \", \" \".join(fr_pred_sent))\n",
    "    \n",
    "def evaluate_bleu_score(encoder, decoder, test_dataset, fr_word2idx, fr_idx2word):\n",
    "    bleu_scores = []\n",
    "    smoothing_fn = SmoothingFunction()\n",
    "    \n",
    "    for encoder_in, decoder_in, decoder_out in test_dataset:\n",
    "        encoder_state = encoder.init_state(batch_size)\n",
    "        encoder_out, encoder_state = encoder(encoder_in, encoder_state)\n",
    "        decoder_state = encoder_state\n",
    "        \n",
    "        ref_sent_ids = np.zeros_like(decoder_out)\n",
    "        hyp_sent_ids = np.zeros_like(decoder_out)\n",
    "        for t in range(decoder_out.shape[1]):\n",
    "            decoder_out_t = decoder_out[:, t]\n",
    "            decoder_in_t = decoder_in[:, t]\n",
    "            decoder_pred_t, decoder_state, alignment = decoder(decoder_in_t, decoder_state, encoder_out)\n",
    "            decoder_pred_t = tf.argmax(decoder_pred_t, axis=-1)\n",
    "            for b in range(decoder_out.shape[0]):\n",
    "                ref_sent_ids[b, t] = decoder_out_t.numpy()[0]\n",
    "                hyp_sent_ids[b, t] = decoder_pred_t.numpy()[0][0]\n",
    "        \n",
    "        for b in range(ref_sent_ids.shape[0]):\n",
    "            ref_sent = [fr_idx2word[i] for i in ref_sent_ids[b] if i > 0] \n",
    "            hyp_sent = [fr_idx2word[i] for i in ref_sent_ids[b] if i > 0]\n",
    "            ref_sent = ref_sent[0:-1]\n",
    "            hyp_sent = hyp_sent[0:-1]\n",
    "            \n",
    "            bleu_score = sentence_bleu([ref_sent], hyp_sent, smoothing_function=smoothing_fn.method1)\n",
    "            bleu_scores.append(bleu_score)\n",
    "    \n",
    "    return np.mean(np.array(bleu_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "balanced-england",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============================\n",
      "EPOCH: 1, LOSS: 1.5176\n",
      "------------------------------\n",
      "Input      :     he slept all day .\n",
      "Label      :     il a dormi toute la journee . EOS\n",
      "Predicted  :     tom est un air . EOS\n",
      "EVAL (BLEU):     9.316e-01\n",
      "------------------------------\n",
      "==============================\n",
      "EPOCH: 2, LOSS: 1.2458\n",
      "------------------------------\n",
      "Input      :     we re resilient .\n",
      "Label      :     nous sommes endurantes . EOS\n",
      "Predicted  :     nous sommes tres . EOS\n",
      "EVAL (BLEU):     8.967e-01\n",
      "------------------------------\n",
      "==============================\n",
      "EPOCH: 3, LOSS: 1.0178\n",
      "------------------------------\n",
      "Input      :     get away !\n",
      "Label      :     cassez vous . EOS\n",
      "Predicted  :     veuillez ! EOS\n",
      "EVAL (BLEU):     9.320e-01\n",
      "------------------------------\n",
      "==============================\n",
      "EPOCH: 4, LOSS: 0.9991\n",
      "------------------------------\n",
      "Input      :     fix the clock .\n",
      "Label      :     reparez l horloge . EOS\n",
      "Predicted  :     laisse le monde ! EOS\n",
      "EVAL (BLEU):     9.072e-01\n",
      "------------------------------\n",
      "==============================\n",
      "EPOCH: 5, LOSS: 0.9447\n",
      "------------------------------\n",
      "Input      :     nice to meet you .\n",
      "Label      :     enchante de faire votre connaissance . EOS\n",
      "Predicted  :     quelle journee ! EOS\n",
      "EVAL (BLEU):     9.264e-01\n",
      "------------------------------\n",
      "==============================\n",
      "EPOCH: 6, LOSS: 0.9002\n",
      "------------------------------\n",
      "Input      :     did you send them ?\n",
      "Label      :     les avez vous envoyes ? EOS\n",
      "Predicted  :     as tu le croire ? EOS\n",
      "EVAL (BLEU):     9.092e-01\n",
      "------------------------------\n",
      "==============================\n",
      "EPOCH: 7, LOSS: 0.6833\n",
      "------------------------------\n",
      "Input      :     this isn t mine .\n",
      "Label      :     ce n est pas la mienne . EOS\n",
      "Predicted  :     ce n est pas a le faire . EOS\n",
      "EVAL (BLEU):     9.036e-01\n",
      "------------------------------\n",
      "==============================\n",
      "EPOCH: 8, LOSS: 0.9069\n",
      "------------------------------\n",
      "Input      :     wait .\n",
      "Label      :     attendez . EOS\n",
      "Predicted  :     arretez de . EOS\n",
      "EVAL (BLEU):     9.316e-01\n",
      "------------------------------\n",
      "==============================\n",
      "EPOCH: 9, LOSS: 0.6327\n",
      "------------------------------\n",
      "Input      :     you re too skinny .\n",
      "Label      :     vous etes trop maigre . EOS\n",
      "Predicted  :     vous etes trop dur . EOS\n",
      "EVAL (BLEU):     9.092e-01\n",
      "------------------------------\n",
      "==============================\n",
      "EPOCH: 10, LOSS: 0.7326\n",
      "------------------------------\n",
      "Input      :     got it ?\n",
      "Label      :     compris ? EOS\n",
      "Predicted  :     est ce que c est la ? EOS\n",
      "EVAL (BLEU):     8.939e-01\n",
      "------------------------------\n",
      "==============================\n",
      "EPOCH: 11, LOSS: 0.5548\n",
      "------------------------------\n",
      "Input      :     i want a knife .\n",
      "Label      :     je veux un couteau . EOS\n",
      "Predicted  :     je veux un ordinateur portable . EOS\n",
      "EVAL (BLEU):     9.055e-01\n",
      "------------------------------\n",
      "==============================\n",
      "EPOCH: 12, LOSS: 0.4944\n",
      "------------------------------\n",
      "Input      :     my joints ache .\n",
      "Label      :     mes articulations me font mal . EOS\n",
      "Predicted  :     mon nom etait a nouveau . EOS\n",
      "EVAL (BLEU):     9.276e-01\n",
      "------------------------------\n",
      "==============================\n",
      "EPOCH: 13, LOSS: 0.4014\n",
      "------------------------------\n",
      "Input      :     thank you .\n",
      "Label      :     merci . EOS\n",
      "Predicted  :     merci de toi . EOS\n",
      "EVAL (BLEU):     9.207e-01\n",
      "------------------------------\n",
      "==============================\n",
      "EPOCH: 14, LOSS: 0.4356\n",
      "------------------------------\n",
      "Input      :     how tragic !\n",
      "Label      :     c est tragique ! EOS\n",
      "Predicted  :     quelle humiliation ! EOS\n",
      "EVAL (BLEU):     9.307e-01\n",
      "------------------------------\n",
      "==============================\n",
      "EPOCH: 15, LOSS: 0.4052\n",
      "------------------------------\n",
      "Input      :     i read the book .\n",
      "Label      :     je lis le livre . EOS\n",
      "Predicted  :     j ai lu le livre . EOS\n",
      "EVAL (BLEU):     9.207e-01\n",
      "------------------------------\n",
      "==============================\n",
      "EPOCH: 16, LOSS: 0.2984\n",
      "------------------------------\n",
      "Input      :     i m coming .\n",
      "Label      :     j arrive . EOS\n",
      "Predicted  :     je me rends . EOS\n",
      "EVAL (BLEU):     9.404e-01\n",
      "------------------------------\n",
      "==============================\n",
      "EPOCH: 17, LOSS: 0.4954\n",
      "------------------------------\n",
      "Input      :     get me out of here .\n",
      "Label      :     sors moi de la ! EOS\n",
      "Predicted  :     sors moi de l eau ! EOS\n",
      "EVAL (BLEU):     9.223e-01\n",
      "------------------------------\n",
      "==============================\n",
      "EPOCH: 18, LOSS: 0.4885\n",
      "------------------------------\n",
      "Input      :     i hate you .\n",
      "Label      :     je te deteste . EOS\n",
      "Predicted  :     je vous deteste . EOS\n",
      "EVAL (BLEU):     8.927e-01\n",
      "------------------------------\n",
      "==============================\n",
      "EPOCH: 19, LOSS: 0.4293\n",
      "------------------------------\n",
      "Input      :     i heard him go out .\n",
      "Label      :     je l ai entendu sortir . EOS\n",
      "Predicted  :     je l ai entendu y aller . EOS\n",
      "EVAL (BLEU):     9.076e-01\n",
      "------------------------------\n",
      "==============================\n",
      "EPOCH: 20, LOSS: 0.2884\n",
      "------------------------------\n",
      "Input      :     tom plays hockey .\n",
      "Label      :     tom pratique le hockey . EOS\n",
      "Predicted  :     tom joue le hockey . EOS\n",
      "EVAL (BLEU):     8.911e-01\n",
      "------------------------------\n",
      "==============================\n",
      "EPOCH: 21, LOSS: 0.2836\n",
      "------------------------------\n",
      "Input      :     i could try .\n",
      "Label      :     je pourrais essayer . EOS\n",
      "Predicted  :     je pourrais essayer . EOS\n",
      "EVAL (BLEU):     9.276e-01\n",
      "------------------------------\n",
      "==============================\n",
      "EPOCH: 22, LOSS: 0.3029\n",
      "------------------------------\n",
      "Input      :     the water is icy .\n",
      "Label      :     l eau est glacee . EOS\n",
      "Predicted  :     l eau est glacee . EOS\n",
      "EVAL (BLEU):     9.204e-01\n",
      "------------------------------\n",
      "==============================\n",
      "EPOCH: 23, LOSS: 0.3688\n",
      "------------------------------\n",
      "Input      :     that s my opinion .\n",
      "Label      :     c est mon opinion . EOS\n",
      "Predicted  :     c est mon opinion . EOS\n",
      "EVAL (BLEU):     9.104e-01\n",
      "------------------------------\n",
      "==============================\n",
      "EPOCH: 24, LOSS: 0.3002\n",
      "------------------------------\n",
      "Input      :     tom is panting .\n",
      "Label      :     tom halete . EOS\n",
      "Predicted  :     tom a des plans . EOS\n",
      "EVAL (BLEU):     8.973e-01\n",
      "------------------------------\n",
      "==============================\n",
      "EPOCH: 25, LOSS: 0.2203\n",
      "------------------------------\n",
      "Input      :     it s dangerous !\n",
      "Label      :     c est dangereux ! EOS\n",
      "Predicted  :     c est dangereux ! EOS\n",
      "EVAL (BLEU):     9.251e-01\n",
      "------------------------------\n",
      "==============================\n",
      "EPOCH: 26, LOSS: 0.2533\n",
      "------------------------------\n",
      "Input      :     tom is worried .\n",
      "Label      :     tom est inquiet . EOS\n",
      "Predicted  :     tom est inquiet . EOS\n",
      "EVAL (BLEU):     8.971e-01\n",
      "------------------------------\n",
      "==============================\n",
      "EPOCH: 27, LOSS: 0.2399\n",
      "------------------------------\n",
      "Input      :     you re skinny .\n",
      "Label      :     tu es maigrichon . EOS\n",
      "Predicted  :     vous etes maigrichon . EOS\n",
      "EVAL (BLEU):     8.916e-01\n",
      "------------------------------\n",
      "==============================\n",
      "EPOCH: 28, LOSS: 0.2106\n",
      "------------------------------\n",
      "Input      :     i need a coat .\n",
      "Label      :     il me faut un manteau . EOS\n",
      "Predicted  :     j ai besoin d un manteau . EOS\n",
      "EVAL (BLEU):     9.236e-01\n",
      "------------------------------\n",
      "==============================\n",
      "EPOCH: 29, LOSS: 0.1678\n",
      "------------------------------\n",
      "Input      :     sit tight .\n",
      "Label      :     restez assise . EOS\n",
      "Predicted  :     restez assise . EOS\n",
      "EVAL (BLEU):     9.051e-01\n",
      "------------------------------\n",
      "==============================\n",
      "EPOCH: 30, LOSS: 0.1466\n",
      "------------------------------\n",
      "Input      :     congratulations !\n",
      "Label      :     tous mes voeux ! EOS\n",
      "Predicted  :     felicitations ! EOS\n",
      "EVAL (BLEU):     8.988e-01\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "num_epochs = NUM_EPOCHS\n",
    "\n",
    "for e in range(num_epochs):\n",
    "    encoder_state = encoder.init_state(batch_size)\n",
    "    \n",
    "    for batch, data in enumerate(train_dataset):\n",
    "        encoder_in, decoder_in, decoder_out = data\n",
    "        loss = train_step(encoder_in, decoder_in, decoder_out, encoder_state)\n",
    "    \n",
    "    print(\"=\"*30)\n",
    "    print(\"EPOCH: {}, LOSS: {:.4f}\".format(e+1, loss.numpy()))\n",
    "    print(\"-\"*30)\n",
    "    \n",
    "    predict(encoder, decoder, batch_size, en_sents, en_data, fr_sents_out, fr_word2idx, fr_idx2word)\n",
    "    eval_score = evaluate_bleu_score(encoder, decoder, test_dataset, fr_word2idx, fr_idx2word)\n",
    "    print(\"EVAL (BLEU):    \", \"{:.3e}\".format(eval_score))\n",
    "    print(\"-\"*30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "diverse-preview",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
