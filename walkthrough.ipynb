{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "congressional-opinion",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import re \n",
    "import os \n",
    "import unicodedata\n",
    "import zipfile\n",
    "import data_load"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cheap-institution",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "alleged-guitar",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning directory data...\n"
     ]
    }
   ],
   "source": [
    "data_load.clean_dir('data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "prerequisite-natural",
   "metadata": {},
   "outputs": [],
   "source": [
    "download_url = \"http://www.manythings.org/anki/fra-eng.zip\"\n",
    "en_sents, fr_sents_in, fr_sents_out = data_load.download_and_read_url(download_url, num_sent_pairs=10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "loose-drove",
   "metadata": {},
   "source": [
    "### Attention layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "amino-reach",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdditiveAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_units):\n",
    "        super(AdditiveAttention, self).__init__()\n",
    "        self.W1 = tf.keras.layers.Dense(num_units)\n",
    "        self.W2 = tf.keras.layers.Dense(num_units)\n",
    "        self.V  = tf.keras.layers.Dense(1)\n",
    "        \n",
    "    def call(self, query, values):\n",
    "        query_with_timesteps = tf.expand_dims(query, axis=1)\n",
    "        score = self.V(\n",
    "            tf.keras.activations.tanh(self.W1(query_with_timesteps) + self.W2(values))\n",
    "        )\n",
    "        alignment = tf.nn.softmax(score, axis=1)\n",
    "        context = tf.reduce_sum(\n",
    "            tf.linalg.matmul(\n",
    "                tf.linalg.matrix_transpose(alignment),\n",
    "                values\n",
    "            ), axis=1\n",
    "        )\n",
    "        context = tf.expand_dims(context, axis=1)\n",
    "        return context, alignment \n",
    "    \n",
    "class Encoder(tf.keras.models.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, num_timesteps, encoder_dim, **kwargs):\n",
    "        super(Encoder, self).__init__(**kwargs)\n",
    "        self.encoder_dim = encoder_dim\n",
    "        self.embedding = tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=num_timesteps)\n",
    "        self.rnn = tf.keras.layers.GRU(encoder_dim, return_sequences=True, return_state=True)\n",
    "        \n",
    "    def call(self, x, state):\n",
    "        x = self.embedding(x)\n",
    "        x, state = self.rnn(x, initial_state=state)\n",
    "        return x, state\n",
    "    \n",
    "    def init_state(self, batch_size):\n",
    "        return tf.zeros((batch_size, self.encoder_dim))\n",
    "    \n",
    "class Decoder(tf.keras.models.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, num_timesteps, decoder_dim, **kwargs):\n",
    "        super(Decoder, self).__init__(**kwargs)\n",
    "        self.decoder_dim = decoder_dim\n",
    "        self.embedding = tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=num_timesteps)\n",
    "        self.attention = AdditiveAttention(embedding_dim)\n",
    "        self.rnn = tf.keras.layers.GRU(decoder_dim, return_sequences=True, return_state=True)\n",
    "        self.Wc = tf.keras.layers.Dense(decoder_dim, activation='tanh')\n",
    "        self.Ws = tf.keras.layers.Dense(vocab_size)\n",
    "        \n",
    "    def call(self, x, state, encoder_out):\n",
    "        x = self.embedding(x)\n",
    "        context, alignment = self.attention(x, encoder_out)\n",
    "        x = tf.expand_dims(\n",
    "            tf.concat([\n",
    "                x, tf.squeeze(context, axis=1)\n",
    "            ], axis=1),\n",
    "        axis=1)\n",
    "        x, state = self.rnn(x, state)\n",
    "        x = self.Wc(x)\n",
    "        x = self.Ws(x)\n",
    "        return x, state, alignment\n",
    "    \n",
    "def loss_fn(ytrue, ypred):\n",
    "    scce = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "    mask = tf.math.logical_not(tf.math.equal(ytrue, 0))\n",
    "    mask = tf.cast(mask, dtype=tf.int64)\n",
    "    loss = scce(ytrue, ypred, sample_weight=mask)\n",
    "    return loss\n",
    "\n",
    "@tf.function\n",
    "def train_step(encoder_in, decoder_in, decoder_out, encoder_state):\n",
    "    with tf.GradientTape() as tape:\n",
    "        encoder_out, encoder_state = encoder(encoder_in, encoder_state)\n",
    "        decoder_state = encoder_state\n",
    "        \n",
    "        loss = 0\n",
    "        for t in range(decoder_out.shape[1]):\n",
    "            decoder_in_t = decoder_in[:, t]\n",
    "            decoder_pred_t, decoder_state, _ = decoder(decoder_in_t, decoder_state, encoder_out)\n",
    "            loss += loss_fn(decoder_out[:, t], decoder_pred_t)\n",
    "    \n",
    "    trainable_variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "    gradients = tape.gradient(loss, trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, trainable_variables))\n",
    "    return loss / decoder_out.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "graduate-mobility",
   "metadata": {},
   "source": [
    "### Tokenization and data set up "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "sacred-lemon",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seqlen (en): 6, seqlen (fr): 14\n"
     ]
    }
   ],
   "source": [
    "en_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters=\"\", lower=False)\n",
    "en_tokenizer.fit_on_texts(en_sents)\n",
    "en_data = en_tokenizer.texts_to_sequences(en_sents)\n",
    "en_data = tf.keras.preprocessing.sequence.pad_sequences(en_data, padding='post')\n",
    "\n",
    "fr_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters=\"\", lower=False)\n",
    "fr_tokenizer.fit_on_texts(fr_sents_in)\n",
    "fr_tokenizer.fit_on_texts(fr_sents_out)\n",
    "fr_data_in = fr_tokenizer.texts_to_sequences(fr_sents_in)\n",
    "fr_data_in = tf.keras.preprocessing.sequence.pad_sequences(fr_data_in, padding='post')\n",
    "fr_data_out = fr_tokenizer.texts_to_sequences(fr_sents_out)\n",
    "fr_data_out = tf.keras.preprocessing.sequence.pad_sequences(fr_data_out, padding='post')\n",
    "\n",
    "en_maxlen = en_data.shape[1]\n",
    "fr_maxlen = fr_data_out.shape[1]\n",
    "print(f\"seqlen (en): {en_maxlen}, seqlen (fr): {fr_maxlen}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "indonesian-terrace",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_SENT_PAIRS = 10000\n",
    "EMBEDDING_DIM = 32\n",
    "ENCODER_DIM, DECODER_DIM = 64, 64\n",
    "BATCH_SIZE = 16\n",
    "NUM_EPOCHS = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "running-marks",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab size (en): 1988, vocab size (fr): 3758\n"
     ]
    }
   ],
   "source": [
    "tf.random.set_seed(123)\n",
    "\n",
    "batch_size = BATCH_SIZE\n",
    "dataset = tf.data.Dataset.from_tensor_slices((en_data, fr_data_in, fr_data_out))\n",
    "dataset = dataset.shuffle(10000)\n",
    "test_size = NUM_SENT_PAIRS // 4\n",
    "test_dataset = dataset.take(test_size).batch(batch_size, drop_remainder=True)\n",
    "train_dataset = dataset.skip(test_size).batch(batch_size, drop_remainder=True)\n",
    "\n",
    "en_vocab_size = len(en_tokenizer.word_index)\n",
    "fr_vocab_size = len(fr_tokenizer.word_index)\n",
    "en_word2idx = en_tokenizer.word_index\n",
    "en_idx2word = {idx:word for word, idx in en_word2idx.items()}\n",
    "fr_word2idx = fr_tokenizer.word_index\n",
    "fr_idx2word = {idx:word for word, idx in fr_word2idx.items()}\n",
    "print(f\"vocab size (en): {en_vocab_size}, vocab size (fr): {fr_vocab_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "seventh-oriental",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder(en_vocab_size+1, embedding_dim=EMBEDDING_DIM, num_timesteps=en_maxlen, encoder_dim=ENCODER_DIM)\n",
    "decoder = Decoder(fr_vocab_size+1, embedding_dim=EMBEDDING_DIM, num_timesteps=fr_maxlen, decoder_dim=DECODER_DIM)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fleet-leone",
   "metadata": {},
   "source": [
    "### Test code to examine dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "connected-ultimate",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs: (16, 6) (16, 14) (16, 14)\n",
      "encoder input          : (16, 6)\n",
      "encoder output         : (16, 6, 64) state: (16, 64)\n",
      "decoder output (logits): (14, 16, 3759) state: (16, 64)\n",
      "decoder output (labels): (16, 14)\n"
     ]
    }
   ],
   "source": [
    "for encoder_in, decoder_in, decoder_out in train_dataset:\n",
    "    print(\"inputs:\", encoder_in.shape, decoder_in.shape, decoder_out.shape)\n",
    "    encoder_state = encoder.init_state(batch_size)\n",
    "    encoder_out, encoder_state = encoder(encoder_in, encoder_state)\n",
    "    decoder_state = encoder_state\n",
    "    decoder_pred = []\n",
    "    for t in range(decoder_out.shape[1]):\n",
    "        decoder_in_t = decoder_in[:, t]\n",
    "        decoder_pred_t, decoder_state, _ = decoder(decoder_in_t,\n",
    "            decoder_state, encoder_out)\n",
    "        decoder_pred.append(decoder_pred_t.numpy())\n",
    "    decoder_pred = tf.squeeze(np.array(decoder_pred), axis=2)\n",
    "    break\n",
    "print(\"encoder input          :\", encoder_in.shape)\n",
    "print(\"encoder output         :\", encoder_out.shape, \"state:\", encoder_state.shape)\n",
    "print(\"decoder output (logits):\", decoder_pred.shape, \"state:\", decoder_state.shape)\n",
    "print(\"decoder output (labels):\", decoder_out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "billion-logan",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "hazardous-rider",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 1, LOSS: 1.6801\n",
      "EPOCH: 2, LOSS: 1.3345\n",
      "EPOCH: 3, LOSS: 1.2794\n"
     ]
    }
   ],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "num_epochs = NUM_EPOCHS\n",
    "\n",
    "for e in range(num_epochs):\n",
    "    encoder_state = encoder.init_state(batch_size)\n",
    "    \n",
    "    for batch, data in enumerate(train_dataset):\n",
    "        encoder_in, decoder_in, decoder_out = data\n",
    "        loss = train_step(encoder_in, decoder_in, decoder_out, encoder_state)\n",
    "        \n",
    "    print(\"EPOCH: {}, LOSS: {:.4f}\".format(e+1, loss.numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "oriental-trading",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
